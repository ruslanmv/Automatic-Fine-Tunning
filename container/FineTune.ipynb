{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06ce1c4-3d3d-4ad5-9893-27065c290b96",
   "metadata": {},
   "source": [
    "# ruslanmv/Automatic-Fine-Tunning\n",
    "## For SageMaker/ Google Colab\n",
    "\n",
    "In this notebook we are going to Fine Tune the Mixtral Model adapted to ai-medical-chatbot with more than 250k of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408f1b5-3d83-4697-83f4-bb1bf8dbf2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil  # Install using `!pip install psutil` if needed\n",
    "def get_cpu_and_memory_info():\n",
    "    \"\"\"Retrieves CPU usage (percentage) and total memory (GB) information.\n",
    "    Returns:\n",
    "        dict: A dictionary containing CPU usage and total memory information.\n",
    "    \"\"\"\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)  # Sample CPU usage every second\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "\n",
    "    return {\"cpu_usage\": cpu_usage, \"total_memory\": total_memory}\n",
    "\n",
    "# Get CPU and memory information\n",
    "info = get_cpu_and_memory_info()\n",
    "\n",
    "print(f\"CPU Usage: {info['cpu_usage']}%\")\n",
    "print(f\"Total Memory: {info['total_memory']:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd264cc1-8e29-4fc1-8dc0-06846aaf58dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import psutil\n",
    "def get_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        return gpu_count, gpu_name\n",
    "    else:\n",
    "        return 0, \"No GPU available\"\n",
    "# Get GPU information\n",
    "gpu_count, gpu_name = get_gpu_info()\n",
    "print(f\"Number of GPUs: {gpu_count}\")\n",
    "print(f\"GPU Name: {gpu_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e0b48-12b9-4ca1-8eda-617fe3f21be2",
   "metadata": {},
   "source": [
    "## Step 1 - Identification of Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419fa98-f56f-42ea-8cdc-a88c9632b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from IPython.core.display import get_ipython\n",
    "  is_colab =  get_ipython() is not None and get_ipython().get_fullname() == '__main__'\n",
    "except:\n",
    "  is_colab = False\n",
    "if is_colab:\n",
    "    print(\"You are on Google Colab!\")\n",
    "else:\n",
    "    print(\"You are not on Google Colab.\")\n",
    "    try:\n",
    "        import boto3\n",
    "        # Assuming you have IAM permissions to list SageMaker notebook instances\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        response = sagemaker_client.list_notebook_instances()\n",
    "        # Check if any notebook instances are listed\n",
    "        if len(response['NotebookInstances']) > 0:\n",
    "            print(\"You are on SageMaker notebook instance.\")\n",
    "            is_sagemaker=True\n",
    "        else:\n",
    "            print(\"SageMaker API check inconclusive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking with SageMaker API: {e}\")\n",
    "        print(\"Result inconclusive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16083b87-d5a4-452d-baa0-e0622ce87074",
   "metadata": {},
   "source": [
    "## Step 2 - Environment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1514b-e634-4b18-84ba-e77721b7c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if is_colab:\n",
    "    #@markdown # Connect Google Drive\n",
    "    from google.colab import drive\n",
    "    from IPython.display import clear_output\n",
    "    import ipywidgets as widgets\n",
    "    import os\n",
    "    def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
    "    Shared_Drive = \"\" #@param {type:\"string\"}\n",
    "    #@markdown - Leave empty if you're not using a shared drive\n",
    "    print(\"[0;33mConnecting...\")\n",
    "    drive.mount('/content/gdrive')\n",
    "    if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "      mainpth=\"Shareddrives/\"+Shared_Drive\n",
    "    else:\n",
    "      mainpth=\"MyDrive\"\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "else:\n",
    "    env_name = os.environ.get(\"CONDA_DEFAULT_ENV\", \"\")\n",
    "    if env_name == \"conda_pytorch_p310\":\n",
    "        print(\"Not detected Default Pytorch Environment\")\n",
    "        print(\"Installing missing packages\")\n",
    "        !pip3 install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    else:\n",
    "        print(\"Environment name:\", env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e936faa-d31c-41e0-a4eb-15f94e7970f5",
   "metadata": {},
   "source": [
    "## Step 3 - Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7331fcd-837e-4150-8ee0-6dfa03e6e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if is_colab:\n",
    "    #@markdown # Install/Update ruslanmv repo\n",
    "    from IPython.utils import capture\n",
    "    from IPython.display import clear_output\n",
    "    from subprocess import getoutput\n",
    "    import ipywidgets as widgets\n",
    "    import sys\n",
    "    import fileinput\n",
    "    import os\n",
    "    import time\n",
    "    import base64\n",
    "    import requests\n",
    "    from urllib.request import urlopen, Request\n",
    "    from urllib.parse import urlparse, parse_qs, unquote\n",
    "    from tqdm import tqdm\n",
    "    import six\n",
    "    blsaphemy = base64.b64decode(\"ZWJ1aQ==\").decode('ascii')\n",
    "    if not os.path.exists(\"/content/gdrive\"):\n",
    "        print('\\033[1;31mGdrive not connected, using temporary colab storage ...')\n",
    "        time.sleep(4)\n",
    "        mainpth = \"MyDrive\"\n",
    "        !mkdir -p /content/gdrive/$mainpth\n",
    "        Shared_Drive = \"\"\n",
    "\n",
    "    if Shared_Drive != \"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "        print('\\033[1;31mShared drive not detected, using default MyDrive')\n",
    "        mainpth = \"MyDrive\"\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        def inf(msg, style, wdth):\n",
    "            inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
    "            display(inf)\n",
    "        fgitclone = \"git clone --depth 1\"\n",
    "        !mkdir -p /content/gdrive/$mainpth/llm\n",
    "        # Define the path\n",
    "        main_path =f\"/content/gdrive/{mainpth}/\"\n",
    "        !git clone -q --branch master https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/$mainpth/llm/Automatic-Fine-Tunning\n",
    "        os.environ['TRANSFORMERS_CACHE'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        cache_dir = os.environ['TRANSFORMERS_CACHE']\n",
    "        !mkdir -p /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories\n",
    "        !git clone https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        %cd /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "        !git reset --hard\n",
    "        !git checkout master\n",
    "        time.sleep(1)\n",
    "        !git pull\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done', 'success', '50px')\n",
    "    #@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f82eb4-08a4-4442-995a-effd278ebfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reload_environment():\n",
    "    # Kernel restart logic (may not work consistently within Jupyter Notebook)\n",
    "    try:\n",
    "      from IPython import get_ipython\n",
    "      get_ipython().kernel.do_shutdown(restart=True)\n",
    "      print(\"Kernel restarted. Packages should be reloaded.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Kernel restart failed: {e}\")\n",
    "      print(\"Consider manually restarting the kernel or your Jupyter Notebook server.\")\n",
    "if is_colab:\n",
    "    #@markdown # Requirements\n",
    "    print('[1;32mInstalling requirements...')\n",
    "    with capture.capture_output() as cap:\n",
    "      %cd /content/\n",
    "      !wget -q -i https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt\n",
    "      !pip install -r requirements.txt\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "if is_sagemaker:\n",
    "    !pip install -qU transformers==4.36.2 accelerate==0.25.0 duckduckgo_search==4.1.0  python-dotenv\n",
    "    !pip install -qU  bitsandbytes transformers==4.36.2 peft accelerate trl datasets==2.16.0 sentencepiece protobuf\n",
    "    !wget -q https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt -O requirements.txt\n",
    "    !pip install -qU -r requirements.txt \n",
    "    #reload_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228279-6247-43dc-9c69-030192a28322",
   "metadata": {},
   "source": [
    "##  Step 4  - Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5174a2-6cd3-4f18-a9b8-701da2137ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b881bfa-121a-43a3-abd6-d44cd47b131c",
   "metadata": {},
   "source": [
    "## Step 5 - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db910d7-5b33-42fd-a104-2f968c0cb53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base model from huggingFace or path to model\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# New model name\n",
    "\n",
    "new_model = \"Medical-Mixtral-7B-v250k\"\n",
    "num=256500\n",
    "\n",
    "#new_model = \"Medical-Mixtral-7B-v1k\"\n",
    "#num=1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41088a83-f6a9-4820-8241-0400a05113d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##  Loading Data\n",
    "# Access the environment variable\n",
    "if is_colab:\n",
    "    from google.colab import userdata\n",
    "    from google.colab import userdata\n",
    "    secret_hf = userdata.get('HF_TOKEN')\n",
    "else:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    # Check if .env file exists\n",
    "    if not os.path.exists('.env'):\n",
    "        # Print the URL for Hugging Face token\n",
    "        print(\"Please go to the following URL and obtain your Hugging Face token:\")\n",
    "        print(\"https://huggingface.co/settings/tokens\")\n",
    "        print()\n",
    "        # Prompt user to enter HF_TOKEN manually\n",
    "        hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "        # Create or append to .env file\n",
    "        with open('.env', 'a') as f:\n",
    "            f.write(f\"HF_TOKEN={hf_token}\\n\")\n",
    "\n",
    "    # Load the .env file\n",
    "    load_dotenv()\n",
    "    # Retrieve the value of HF_TOKEN from the environment variables\n",
    "    secret_hf = os.environ.get('HF_TOKEN')\n",
    "    # Clear output to hide the token\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    # Print the value of HF_TOKEN\n",
    "    print(\"Loaded HF Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5257211-b39d-4843-ac33-c7a990fde80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then you can use the token in your command\n",
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619f3a5-0b94-4b0b-930a-2aa3447e7d7a",
   "metadata": {},
   "source": [
    "## Step 6 -  Datataset to Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e69ee-b9d1-4431-84bb-6b682cebb212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "train_data = dataset_[\"train\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(train_data[::])\n",
    "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"question\", \"Doctor\": \"answer\"})\n",
    "# Clean the question and answer columns\n",
    "df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "# Assuming your DataFrame is named 'df' and the column is named 'df' and the column is named 'question'\n",
    "df['question'] = df['question'].str.lstrip('Q. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5210724-2cd6-4955-8754-11a51b322457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ac366-a6f9-4ad2-8f5e-44f328d66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:num, :]\n",
    "df_test = df.iloc[num:num+100, :]\n",
    "# Save the train dataframe to a CSV file\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "# Save the test dataframe to a CSV file\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "df=df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3405fc-59a1-4c6f-bf08-018f52be3a40",
   "metadata": {},
   "source": [
    "## Step 7 - Formatting Your Fine-tuning Data\n",
    "There are various ways to format your data for fine-tuning\n",
    "\n",
    "Prompts provide context and guide the LLM towards the desired task. The code showcases creating prompts for question-answering tasks with placeholders for questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d12f6a-a784-4cee-a260-37014d689c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build training dataset with the right format\n",
    "df['text'] = '[INST]@Enlighten. ' + df['question'] +'[/INST]'+ df['answer'] + ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21649658-4ea1-488e-9911-928bd159ba95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove columns\n",
    "df=df.drop(['question','answer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7382ad6-5515-4c7f-a725-58b5c07d43f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f72c6-1ec7-4de2-82d1-941ec3f824db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba91adc-a68a-476b-a17a-ddfdb67c56bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to dataset object\n",
    "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
    "dataset = Dataset(pa.Table.from_pandas(df))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4da5b-43ad-4c62-b25a-4aaf703aeff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fbcbf1-997c-479b-9936-10dfb3ef1c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics=True\n",
    "#dataset_sample = dataset.select(range(500)) # Take only the first 500 records from the dataset\n",
    "if statistics: \n",
    "    sequence_lengths = [len(example['text']) for example in dataset]\n",
    "    # Calculate statistics\n",
    "    max_length = max(sequence_lengths)\n",
    "    min_length = min(sequence_lengths)\n",
    "    mean_length = sum(sequence_lengths) / len(sequence_lengths)\n",
    "    median_length = sorted(sequence_lengths)[len(sequence_lengths) // 2]\n",
    "    \n",
    "    print(\"Max length:\", max_length)\n",
    "    print(\"Min length:\", min_length)\n",
    "    print(\"Mean length:\", mean_length)\n",
    "    print(\"Median length:\", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef947cf3-5c34-4b8a-8bea-82f364c98b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_sagemaker:\n",
    "    # Get the current directory and join with the models folder\n",
    "    current_directory = os.getcwd()\n",
    "    cache_dir = os.path.join(current_directory, \"models\")\n",
    "    main_path=current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa098e-4f75-47f6-ac0e-7d03323148f1",
   "metadata": {},
   "source": [
    "## Step 8 -  Downloading and Initializing Mixtral 8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbae860-d582-474a-b348-babfba9abce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119bed9-3bf8-4ab5-9295-c3a9d9571fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022551-c1f7-4f61-916e-d4efdfad9434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, cache_dir=cache_dir)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3735c09-7e48-46a0-a46f-9cda5d3219ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1024  # Adjust this value based on your dataset and GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfe4a6-92cd-450d-87f2-06bea8deb3de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_training=False\n",
    "if count_training:\n",
    "    # count trainging tokens\n",
    "    from transformers import LlamaTokenizer\n",
    "    tokenizer_ = LlamaTokenizer.from_pretrained(\"cognitivecomputations/dolphin-llama2-7b\",\n",
    "                                                cache_dir=cache_dir)\n",
    "    tokens = tokenizer_.tokenize(dataset.to_pandas().to_string())\n",
    "    len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c3f55-4c99-4234-b9ac-e90c3d76d049",
   "metadata": {},
   "source": [
    "## Step 9 - Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce2be-67aa-4de2-8391-7dab57836edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding the adapters in the layers\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c7e38-6127-4b89-9c14-635fc3875de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b64b8-ec90-4689-9f7c-2a4a07940a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_default=True\n",
    "if is_default:\n",
    "    # Setting sft parameters\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        #max_seq_length= None,\n",
    "        max_seq_length=1024,  # Adjust based on your dataset and GPU memory\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing= False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ed137-84d3-49ec-9535-e7e24e23d7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cf914-bf95-47ea-acbe-f4d7a400e374",
   "metadata": {},
   "source": [
    "## Step 10 - Save and push the adapter to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6395b-c4cb-49a6-99f5-ccca98774d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model name\n",
    "new_model = \"Medical-Mixtral-7B-v250k\"\n",
    "# Save the fine-tuned model\n",
    "save_path = os.path.join(main_path, \"llm\", \"Automatic-Fine-Tuning\", \"models\", new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed70892-1533-4a1a-9f8c-b4accdcd74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(save_path)\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ec690-9265-4360-8042-8fca1ea1dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84390f-36a6-4587-991c-4d9014bfea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895ff6a-7210-443d-a02f-da43f5bb3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a Model Card\n",
    "model_card = {\n",
    "  \"Model Name\": \"Medical-Mixtral-7B-250k\",\n",
    "  \"Description\": \"Fine-tuned Mixtral model for answering medical assistance questions. This model is a novel version of mistralai/Mixtral-8x7B-Instruct-v0.1, adapted to a subset of 1.5k records from the AI Medical Chatbot dataset, which contains 250k records (https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot). The purpose of this model is to provide a ready chatbot to answer questions related to medical assistance.\",\n",
    "  \"Intended Use\": \"This model is intended for providing assistance and answering questions related to medical inquiries. It is suitable for use in chatbot applications where users seek medical advice, information, or assistance.\",\n",
    "  \"Example Usage\": \"```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Define the name of your fine-tuned model\\nfinetuned_model = 'ruslanmv/Medical-Assistance-Mixtral-7B-v1.5k'\\n\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(finetuned_model, trust_remote_code=True)\\n\\n# Load the model with the provided adapter configuration and weights\\nmodel_pretrained = AutoModelForCausalLM.from_pretrained(finetuned_model, trust_remote_code=True, torch_dtype=torch.float16, cache_dir=cache_dir)\\n\\nmessages = [\\n    {'role': 'user', 'content': 'What should I do to reduce my weight gained due to genetic hypothyroidism?'},\\n    {'role': 'assistant', 'content': ''},\\n]\\n\\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\\n\\noutputs = model_pretrained.generate(input_ids, max_new_tokens=500)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\",\n",
    "  \"Training Data\": {\n",
    "    \"Dataset Name\": \"AI Medical Chatbot\",\n",
    "    \"Dataset URL\": \"https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot\",\n",
    "    \"Dataset Size\": \"250k records\",\n",
    "    \"Subset Used\": \"250k records\"\n",
    "  },\n",
    "  \"Limitations\": [\n",
    "    \"The model's performance may vary depending on the complexity and specificity of the medical questions.\",\n",
    "    \"The model may not provide accurate answers for every medical query, and users should consult medical professionals for critical healthcare concerns.\"\n",
    "  ],\n",
    "  \"Ethical Considerations\": [\n",
    "    \"Users should be informed that the model's responses are generated based on patterns in the training data and may not always be accurate or suitable for medical decision-making.\",\n",
    "    \"The model should not be used as a replacement for professional medical advice or diagnosis.\",\n",
    "    \"Sensitive patient data should not be shared with the model, and user privacy should be protected.\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Save the Model Card\n",
    "model_card_path = os.path.join(save_path, \"model_card.json\")\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    json.dump(model_card, f)\n",
    "\n",
    "model.push_to_hub(new_model, model_card=model_card_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ef00-22ad-4d9e-9df7-4da73be4262e",
   "metadata": {},
   "source": [
    "## Step 11 - Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996e30e-cdbf-47a1-b929-51bfb004be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c7cf7-277d-4044-b473-06cc96209026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feda9e-0264-4c0d-9914-ee4baff102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does abutment of the nerve root mean?\"\n",
    "prompt = build_prompt(question)\n",
    "result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb7a62-0636-4d6d-a51a-29b4ccb16a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eed049-0ef5-465b-8665-5aa3a725de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_path=\"test.csv\"\n",
    "test_path=\"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bddf8-f0b4-438c-a702-a7131ec14456",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test=pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12660cdc-a758-4d79-a0dc-721f16d3d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65073c7-14a8-4772-94f5-c8d36e97083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We test only 10 entries\n",
    "df_test=df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc51e-9ca7-4018-9a93-82bd560336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3d197-4ce1-4f4b-a729-5d9f122bcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionCounter = 0\n",
    "correct = 0\n",
    "promptEnding = \"[/INST]\"\n",
    "\n",
    "# Guide for answering questions\n",
    "testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df_test.iterrows():\n",
    "    print(\"#############################\")\n",
    "    questionCounter += 1\n",
    "\n",
    "    # Build the question prompt\n",
    "    question = testGuide + row['question'] + \"\\n\"\n",
    "    print(question)\n",
    "\n",
    "    # Get the true answer\n",
    "    truth = row['answer']\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    # Generate answer\n",
    "    result = pipe(prompt)\n",
    "    llmAnswer = result[0]['generated_text']\n",
    "\n",
    "    # Remove the prompt from the generated answer\n",
    "    index = llmAnswer.find(promptEnding)\n",
    "    llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "\n",
    "    print(\"LLM Answer:\")\n",
    "    print(llmAnswer)\n",
    "\n",
    "    # Remove spaces from the generated answer\n",
    "    llmAnswer = llmAnswer.replace(' ', '')\n",
    "\n",
    "    # Find the option in response\n",
    "    index = llmAnswer.find('answer:')\n",
    "\n",
    "    # Find and match the option\n",
    "    next_char = llmAnswer[index + len('answer:'):][0]\n",
    "    if next_char in truth:\n",
    "        correct += 1\n",
    "        print('correct')\n",
    "    else:\n",
    "        print('wrong')\n",
    "\n",
    "    # Update accuracy\n",
    "    accuracy = correct / questionCounter\n",
    "    print(f\"Progress: {questionCounter / len(df_test)}\")\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76bd5e-e8e4-43c0-b4c4-195c15fb757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Medical-Mixtral-7B-v1.5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978cf0f-1297-4da2-9250-b834ffa0af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99118e41-4896-4029-84c6-538459b55191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Define the name of your fine-tuned model\n",
    "finetuned_model = \"ruslanmv/Medical-Mixtral-7B-v250k\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449dc9e-73e8-45d1-9870-9f2a4a12e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88644a-4ec6-4150-837d-8f981d2edf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(\n",
    "        finetuned_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd261dfc-dd05-4312-8173-0018af775a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pretrained.eval()\n",
    "## Text Generation Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58eddf-fac6-4493-867e-ee93e3798013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0d154-0519-4738-a1ad-797785788f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model_pretrained, tokenizer=tokenizer, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c532313-b277-4c39-89c2-86fa5c64e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "  promptEnding = \"[/INST]\"\n",
    "  # Guide for answering questions\n",
    "  testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "  # Build the question prompt\n",
    "  question = testGuide + question + \"\\n\"\n",
    "  print(question)\n",
    "  # Build the prompt\n",
    "  prompt = build_prompt(question)\n",
    "  # Generate answer\n",
    "  result = pipe(prompt)\n",
    "  llmAnswer = result[0]['generated_text']\n",
    "  # Remove the prompt from the generated answer\n",
    "  index = llmAnswer.find(promptEnding)\n",
    "  llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "  print(\"LLM Answer:\")\n",
    "  print(llmAnswer)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd3f55-c8f3-468e-8d2b-64be141539fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"My symptoms after intercourse threatns me even after having negative HIV result. Kindly help.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280d8d6-8ff9-4117-b1a1-ba592451ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
