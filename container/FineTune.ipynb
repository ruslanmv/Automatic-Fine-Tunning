{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06ce1c4-3d3d-4ad5-9893-27065c290b96",
   "metadata": {},
   "source": [
    "# ruslanmv/Automatic-Fine-Tunning\n",
    "## For SageMaker/ Google Colab\n",
    "\n",
    "In this notebook we are going to Fine Tune the Mixtral Model adapted to ai-medical-chatbot with more than 250k of records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f408f1b5-3d83-4697-83f4-bb1bf8dbf2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Usage: 0.0%\n",
      "Total Memory: 1121.81 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil  # Install using `!pip install psutil` if needed\n",
    "def get_cpu_and_memory_info():\n",
    "    \"\"\"Retrieves CPU usage (percentage) and total memory (GB) information.\n",
    "    Returns:\n",
    "        dict: A dictionary containing CPU usage and total memory information.\n",
    "    \"\"\"\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)  # Sample CPU usage every second\n",
    "    total_memory = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "\n",
    "    return {\"cpu_usage\": cpu_usage, \"total_memory\": total_memory}\n",
    "\n",
    "# Get CPU and memory information\n",
    "info = get_cpu_and_memory_info()\n",
    "\n",
    "print(f\"CPU Usage: {info['cpu_usage']}%\")\n",
    "print(f\"Total Memory: {info['total_memory']:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd264cc1-8e29-4fc1-8dc0-06846aaf58dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 8\n",
      "GPU Name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "def get_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        return gpu_count, gpu_name\n",
    "    else:\n",
    "        return 0, \"No GPU available\"\n",
    "# Get GPU information\n",
    "gpu_count, gpu_name = get_gpu_info()\n",
    "print(f\"Number of GPUs: {gpu_count}\")\n",
    "print(f\"GPU Name: {gpu_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e0b48-12b9-4ca1-8eda-617fe3f21be2",
   "metadata": {},
   "source": [
    "## Step 1 - Identification of Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c419fa98-f56f-42ea-8cdc-a88c9632b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are not on Google Colab.\n",
      "You are on SageMaker notebook instance.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from IPython.core.display import get_ipython\n",
    "  is_colab =  get_ipython() is not None and get_ipython().get_fullname() == '__main__'\n",
    "except:\n",
    "  is_colab = False\n",
    "if is_colab:\n",
    "    print(\"You are on Google Colab!\")\n",
    "else:\n",
    "    print(\"You are not on Google Colab.\")\n",
    "    try:\n",
    "        import boto3\n",
    "        # Assuming you have IAM permissions to list SageMaker notebook instances\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        response = sagemaker_client.list_notebook_instances()\n",
    "        # Check if any notebook instances are listed\n",
    "        if len(response['NotebookInstances']) > 0:\n",
    "            print(\"You are on SageMaker notebook instance.\")\n",
    "            is_sagemaker=True\n",
    "        else:\n",
    "            print(\"SageMaker API check inconclusive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking with SageMaker API: {e}\")\n",
    "        print(\"Result inconclusive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16083b87-d5a4-452d-baa0-e0622ce87074",
   "metadata": {},
   "source": [
    "## Step 2 - Environment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c1514b-e634-4b18-84ba-e77721b7c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment name: pytorch_p310\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if is_colab:\n",
    "    #@markdown # Connect Google Drive\n",
    "    from google.colab import drive\n",
    "    from IPython.display import clear_output\n",
    "    import ipywidgets as widgets\n",
    "    import os\n",
    "    def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
    "    Shared_Drive = \"\" #@param {type:\"string\"}\n",
    "    #@markdown - Leave empty if you're not using a shared drive\n",
    "    print(\"[0;33mConnecting...\")\n",
    "    drive.mount('/content/gdrive')\n",
    "    if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "      mainpth=\"Shareddrives/\"+Shared_Drive\n",
    "    else:\n",
    "      mainpth=\"MyDrive\"\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "else:\n",
    "    env_name = os.environ.get(\"CONDA_DEFAULT_ENV\", \"\")\n",
    "    if env_name == \"conda_pytorch_p310\":\n",
    "        print(\"Not detected Default Pytorch Environment\")\n",
    "        print(\"Installing missing packages\")\n",
    "        !pip3 install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    else:\n",
    "        print(\"Environment name:\", env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e936faa-d31c-41e0-a4eb-15f94e7970f5",
   "metadata": {},
   "source": [
    "## Step 3 - Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7331fcd-837e-4150-8ee0-6dfa03e6e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if is_colab:\n",
    "    #@markdown # Install/Update ruslanmv repo\n",
    "    from IPython.utils import capture\n",
    "    from IPython.display import clear_output\n",
    "    from subprocess import getoutput\n",
    "    import ipywidgets as widgets\n",
    "    import sys\n",
    "    import fileinput\n",
    "    import os\n",
    "    import time\n",
    "    import base64\n",
    "    import requests\n",
    "    from urllib.request import urlopen, Request\n",
    "    from urllib.parse import urlparse, parse_qs, unquote\n",
    "    from tqdm import tqdm\n",
    "    import six\n",
    "    blsaphemy = base64.b64decode(\"ZWJ1aQ==\").decode('ascii')\n",
    "    if not os.path.exists(\"/content/gdrive\"):\n",
    "        print('\\033[1;31mGdrive not connected, using temporary colab storage ...')\n",
    "        time.sleep(4)\n",
    "        mainpth = \"MyDrive\"\n",
    "        !mkdir -p /content/gdrive/$mainpth\n",
    "        Shared_Drive = \"\"\n",
    "\n",
    "    if Shared_Drive != \"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "        print('\\033[1;31mShared drive not detected, using default MyDrive')\n",
    "        mainpth = \"MyDrive\"\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        def inf(msg, style, wdth):\n",
    "            inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
    "            display(inf)\n",
    "        fgitclone = \"git clone --depth 1\"\n",
    "        !mkdir -p /content/gdrive/$mainpth/llm\n",
    "        # Define the path\n",
    "        main_path =f\"/content/gdrive/{mainpth}/\"\n",
    "        !git clone -q --branch master https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/$mainpth/llm/Automatic-Fine-Tunning\n",
    "        os.environ['TRANSFORMERS_CACHE'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        cache_dir = os.environ['TRANSFORMERS_CACHE']\n",
    "        !mkdir -p /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories\n",
    "        !git clone https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        %cd /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "        !git reset --hard\n",
    "        !git checkout master\n",
    "        time.sleep(1)\n",
    "        !git pull\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done', 'success', '50px')\n",
    "    #@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f82eb4-08a4-4442-995a-effd278ebfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reload_environment():\n",
    "    # Kernel restart logic (may not work consistently within Jupyter Notebook)\n",
    "    try:\n",
    "      from IPython import get_ipython\n",
    "      get_ipython().kernel.do_shutdown(restart=True)\n",
    "      print(\"Kernel restarted. Packages should be reloaded.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Kernel restart failed: {e}\")\n",
    "      print(\"Consider manually restarting the kernel or your Jupyter Notebook server.\")\n",
    "if is_colab:\n",
    "    #@markdown # Requirements\n",
    "    print('[1;32mInstalling requirements...')\n",
    "    with capture.capture_output() as cap:\n",
    "      %cd /content/\n",
    "      !wget -q -i https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt\n",
    "      !pip install -r requirements.txt\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "if is_sagemaker:\n",
    "    #!pip install -qU transformers==4.36.2 accelerate==0.25.0 duckduckgo_search==4.1.0  python-dotenv\n",
    "    #!pip install -qU  bitsandbytes transformers==4.36.2 peft accelerate trl datasets==2.16.0 sentencepiece protobuf\n",
    "    !wget -q https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt -O requirements.txt\n",
    "    !pip install -qU -r requirements.txt \n",
    "    #reload_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228279-6247-43dc-9c69-030192a28322",
   "metadata": {},
   "source": [
    "##  Step 4  - Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5174a2-6cd3-4f18-a9b8-701da2137ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b881bfa-121a-43a3-abd6-d44cd47b131c",
   "metadata": {},
   "source": [
    "## Step 5 - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db910d7-5b33-42fd-a104-2f968c0cb53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base model from huggingFace or path to model\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#base_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# New model name\n",
    "\n",
    "#new_model = \"Medical-Mixtral-7B-v250k\"\n",
    "#num=256500\n",
    "\n",
    "new_model = \"Medical-Mixtral-7B-v2k\"\n",
    "num=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41088a83-f6a9-4820-8241-0400a05113d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF Token\n"
     ]
    }
   ],
   "source": [
    "##  Loading Data\n",
    "# Access the environment variable\n",
    "if is_colab:\n",
    "    from google.colab import userdata\n",
    "    from google.colab import userdata\n",
    "    secret_hf = userdata.get('HF_TOKEN')\n",
    "else:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    # Check if .env file exists\n",
    "    if not os.path.exists('.env'):\n",
    "        # Print the URL for Hugging Face token\n",
    "        print(\"Please go to the following URL and obtain your Hugging Face token:\")\n",
    "        print(\"https://huggingface.co/settings/tokens\")\n",
    "        print()\n",
    "        # Prompt user to enter HF_TOKEN manually\n",
    "        hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "        # Create or append to .env file\n",
    "        with open('.env', 'a') as f:\n",
    "            f.write(f\"HF_TOKEN={hf_token}\\n\")\n",
    "\n",
    "    # Load the .env file\n",
    "    load_dotenv()\n",
    "    # Retrieve the value of HF_TOKEN from the environment variables\n",
    "    secret_hf = os.environ.get('HF_TOKEN')\n",
    "    # Clear output to hide the token\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    # Print the value of HF_TOKEN\n",
    "    print(\"Loaded HF Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5257211-b39d-4843-ac33-c7a990fde80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Then you can use the token in your command\n",
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619f3a5-0b94-4b0b-930a-2aa3447e7d7a",
   "metadata": {},
   "source": [
    "## Step 6 -  Datataset to Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93e69ee-b9d1-4431-84bb-6b682cebb212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "train_data = dataset_[\"train\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(train_data[::])\n",
    "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"question\", \"Doctor\": \"answer\"})\n",
    "# Clean the question and answer columns\n",
    "df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "# Assuming your DataFrame is named 'df' and the column is named 'df' and the column is named 'question'\n",
    "df['question'] = df['question'].str.lstrip('Q. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5210724-2cd6-4955-8754-11a51b322457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does abutment of the nerve root mean?</td>\n",
       "      <td>Hi. I have gone through your query with dilige...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What should I do to reduce my weight gained du...</td>\n",
       "      <td>Hi. You have really done well with the hypothy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have started to get lots of acne on my face,...</td>\n",
       "      <td>Hi there Acne has multifactorial etiology. Onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I have uncomfortable feeling between th...</td>\n",
       "      <td>Hello. The popping and discomfort what you fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My symptoms after intercourse threatns me even...</td>\n",
       "      <td>Hello. The HIV test uses a finger prick blood ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0         What does abutment of the nerve root mean?   \n",
       "1  What should I do to reduce my weight gained du...   \n",
       "2  I have started to get lots of acne on my face,...   \n",
       "3  Why do I have uncomfortable feeling between th...   \n",
       "4  My symptoms after intercourse threatns me even...   \n",
       "\n",
       "                                              answer  \n",
       "0  Hi. I have gone through your query with dilige...  \n",
       "1  Hi. You have really done well with the hypothy...  \n",
       "2  Hi there Acne has multifactorial etiology. Onl...  \n",
       "3  Hello. The popping and discomfort what you fel...  \n",
       "4  Hello. The HIV test uses a finger prick blood ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1ac366-a6f9-4ad2-8f5e-44f328d66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:num, :]\n",
    "df_test = df.iloc[num:num+100, :]\n",
    "# Save the train dataframe to a CSV file\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "# Save the test dataframe to a CSV file\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "df=df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3405fc-59a1-4c6f-bf08-018f52be3a40",
   "metadata": {},
   "source": [
    "## Step 7 - Formatting Your Fine-tuning Data\n",
    "There are various ways to format your data for fine-tuning\n",
    "\n",
    "Prompts provide context and guide the LLM towards the desired task. The code showcases creating prompts for question-answering tasks with placeholders for questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d12f6a-a784-4cee-a260-37014d689c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build training dataset with the right format\n",
    "df['text'] = '[INST]@Enlighten. ' + df['question'] +'[/INST]'+ df['answer'] + ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21649658-4ea1-488e-9911-928bd159ba95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove columns\n",
    "df=df.drop(['question','answer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7382ad6-5515-4c7f-a725-58b5c07d43f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST]@Enlighten. What does abutment of the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST]@Enlighten. What should I do to reduce m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST]@Enlighten. I have started to get lots o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST]@Enlighten. Why do I have uncomfortable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST]@Enlighten. My symptoms after intercours...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [INST]@Enlighten. What does abutment of the ne...\n",
       "1  [INST]@Enlighten. What should I do to reduce m...\n",
       "2  [INST]@Enlighten. I have started to get lots o...\n",
       "3  [INST]@Enlighten. Why do I have uncomfortable ...\n",
       "4  [INST]@Enlighten. My symptoms after intercours..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "995f72c6-1ec7-4de2-82d1-941ec3f824db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba91adc-a68a-476b-a17a-ddfdb67c56bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataset object\n",
    "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
    "dataset = Dataset(pa.Table.from_pandas(df))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e4da5b-43ad-4c62-b25a-4aaf703aeff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fbcbf1-997c-479b-9936-10dfb3ef1c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 3368\n",
      "Min length: 109\n",
      "Mean length: 647.52\n",
      "Median length: 606\n"
     ]
    }
   ],
   "source": [
    "statistics=True\n",
    "#dataset_sample = dataset.select(range(500)) # Take only the first 500 records from the dataset\n",
    "if statistics: \n",
    "    sequence_lengths = [len(example['text']) for example in dataset]\n",
    "    # Calculate statistics\n",
    "    max_length = max(sequence_lengths)\n",
    "    min_length = min(sequence_lengths)\n",
    "    mean_length = sum(sequence_lengths) / len(sequence_lengths)\n",
    "    median_length = sorted(sequence_lengths)[len(sequence_lengths) // 2]\n",
    "    \n",
    "    print(\"Max length:\", max_length)\n",
    "    print(\"Min length:\", min_length)\n",
    "    print(\"Mean length:\", mean_length)\n",
    "    print(\"Median length:\", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef947cf3-5c34-4b8a-8bea-82f364c98b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_sagemaker:\n",
    "    # Get the current directory and join with the models folder\n",
    "    current_directory = os.getcwd()\n",
    "    cache_dir = os.path.join(current_directory, \"models\")\n",
    "    main_path=current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa098e-4f75-47f6-ac0e-7d03323148f1",
   "metadata": {},
   "source": [
    "## Step 8 -  Downloading and Initializing Mixtral 8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fbae860-d582-474a-b348-babfba9abce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d690f7236f3846e89779648fed16208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c64421d1e8e4db298bf9d580085a73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19cfbff5fbc4f60859e391b0de5f1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56f024176de4eb5b9f23e27b6f29746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1365a4d3d20644b3aaace0d45cf80e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f8a7ba4a65442b95effdf895816689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179a968259b489a8ce418dbc7cb6489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd6924eed02466cac452482b6662364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9119bed9-3bf8-4ab5-9295-c3a9d9571fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c022551-c1f7-4f61-916e-d4efdfad9434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c466d4bb0fe049cead0b4953340550b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72348c2f4ec043cdbafb6062954698b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31821fa4aaf492a9dfc0fc62d796dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875460406952498f98ed9fefe0165d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, cache_dir=cache_dir)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3735c09-7e48-46a0-a46f-9cda5d3219ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1024  # Adjust this value based on your dataset and GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61dfe4a6-92cd-450d-87f2-06bea8deb3de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_training=False\n",
    "if count_training:\n",
    "    # count trainging tokens\n",
    "    from transformers import LlamaTokenizer\n",
    "    tokenizer_ = LlamaTokenizer.from_pretrained(\"cognitivecomputations/dolphin-llama2-7b\",\n",
    "                                                cache_dir=cache_dir)\n",
    "    tokens = tokenizer_.tokenize(dataset.to_pandas().to_string())\n",
    "    len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c3f55-4c99-4234-b9ac-e90c3d76d049",
   "metadata": {},
   "source": [
    "## Step 9 - Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8ce2be-67aa-4de2-8391-7dab57836edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding the adapters in the layers\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e9c7e38-6127-4b89-9c14-635fc3875de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c5b64b8-ec90-4689-9f7c-2a4a07940a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e38078263284cbaa1d8a2e89752273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_default=True\n",
    "if is_default:\n",
    "    # Setting sft parameters\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        #max_seq_length= None,\n",
    "        max_seq_length=1024,  # Adjust based on your dataset and GPU memory\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing= False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a6ed137-84d3-49ec-9535-e7e24e23d7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cf914-bf95-47ea-acbe-f4d7a400e374",
   "metadata": {},
   "source": [
    "## Step 10 - Save and push the adapter to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0c6395b-c4cb-49a6-99f5-ccca98774d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New model name\n",
    "#new_model = \"Medical-Mixtral-7B-v250k\"\n",
    "# Save the fine-tuned model\n",
    "save_path = os.path.join(main_path, \"llm\", \"Automatic-Fine-Tuning\", \"models\", new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ed70892-1533-4a1a-9f8c-b4accdcd74ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(save_path)\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a55ec690-9265-4360-8042-8fca1ea1dc64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bc8197086d412c95416dce2d886ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ruslanmv/Medical-Mixtral-7B-v2k/commit/97bacff355e9e1fdd28734410ae9bb1f7947b232', commit_message='Upload model', commit_description='', oid='97bacff355e9e1fdd28734410ae9bb1f7947b232', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e84390f-36a6-4587-991c-4d9014bfea39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ec2-user/SageMaker/container/llm/Automatic-Fine-Tuning/models/Medical-Mixtral-7B-v2k/tokenizer_config.json',\n",
       " '/home/ec2-user/SageMaker/container/llm/Automatic-Fine-Tuning/models/Medical-Mixtral-7B-v2k/special_tokens_map.json',\n",
       " '/home/ec2-user/SageMaker/container/llm/Automatic-Fine-Tuning/models/Medical-Mixtral-7B-v2k/tokenizer.model',\n",
       " '/home/ec2-user/SageMaker/container/llm/Automatic-Fine-Tuning/models/Medical-Mixtral-7B-v2k/added_tokens.json',\n",
       " '/home/ec2-user/SageMaker/container/llm/Automatic-Fine-Tuning/models/Medical-Mixtral-7B-v2k/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcf20600-e902-4b67-ab7f-d36396e49ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a8e4e9dad40a28da0fdb66105a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ruslanmv/Medical-Mixtral-7B-v2k/commit/27080c48c1efcac6192203a78da94d8be5bd5f86', commit_message='Upload tokenizer', commit_description='', oid='27080c48c1efcac6192203a78da94d8be5bd5f86', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6521eae-83f9-4612-856d-287b9b32321a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creation of te README.md\n",
    "# Example of use\n",
    "example = f'''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the name of your fine-tuned model\n",
    "finetuned_model = 'ruslanmv/{new_model}'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model, trust_remote_code=True)\n",
    "\n",
    "# Load the model with the provided adapter configuration and weights\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(finetuned_model, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "\n",
    "messages = [\n",
    "    {{'role': 'user', 'content': 'What should I do to reduce my weight gained due to genetic hypothyroidism?'}},\n",
    "    {{'role': 'assistant', 'content': ''}},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
    "outputs = model_pretrained.generate(input_ids, max_new_tokens=500)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "'''\n",
    "# Create a Model Card\n",
    "model_card = {\n",
    "    \"Model Name\": \"{}\".format(new_model),\n",
    "    \"Description\": \"Fine-tuned Mixtral model for answering medical assistance questions. This model is a novel version of mistralai/Mixtral-8x7B-Instruct-v0.1, adapted to a subset of {}k records from the AI Medical Chatbot dataset, which contains 250k records (https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot). The purpose of this model is to provide a ready chatbot to answer questions related to medical assistance.\".format(num/1000),\n",
    "    \"Intended Use\": \"This model is intended for providing assistance and answering questions related to medical inquiries. It is suitable for use in chatbot applications where users seek medical advice, information, or assistance.\",\n",
    "    \"Example Usage\": example,\n",
    "    \"Training Data\": {\n",
    "        \"Dataset Name\": \"AI Medical Chatbot\",\n",
    "        \"Dataset URL\": \"https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot\",\n",
    "        \"Dataset Size\": \"250k records\",\n",
    "        \"Subset Used\": \"{}k records\".format(num/1000)\n",
    "    },\n",
    "    \"Limitations\": [\n",
    "        \"The model's performance may vary depending on the complexity and specificity of the medical questions.\",\n",
    "        \"The model may not provide accurate answers for every medical query, and users should consult medical professionals for critical healthcare concerns.\"\n",
    "    ],\n",
    "    \"Ethical Considerations\": [\n",
    "        \"Users should be informed that the model's responses are generated based on patterns in the training data and may not always be accurate or suitable for medical decision-making.\",\n",
    "        \"The model should not be used as a replacement for professional medical advice or diagnosis.\",\n",
    "        \"Sensitive patient data should not be shared with the model, and user privacy should be protected.\"\n",
    "    ]\n",
    "}\n",
    "# Additional information\n",
    "library_name = \"peft\"\n",
    "repository_name = f\"ruslanmv/{new_model}\"  # Specify the repository name here\n",
    "# Generate README content\n",
    "readme_content = f\"\"\"---\n",
    "library_name: {library_name}\n",
    "base_model: {base_model}\n",
    "---\n",
    "\n",
    "# {model_card['Model Name']}\n",
    "\n",
    "## Description\n",
    "{model_card['Description']}\n",
    "\n",
    "## Intended Use\n",
    "{model_card['Intended Use']}\n",
    "\n",
    "## Example Usage\n",
    "```python\n",
    "{model_card['Example Usage']}\n",
    "```\n",
    "## Training Data\n",
    "- **Dataset Name:** {model_card['Training Data']['Dataset Name']}\n",
    "- **Dataset URL:** {model_card['Training Data']['Dataset URL']}\n",
    "- **Dataset Size:** {model_card['Training Data']['Dataset Size']}\n",
    "- **Subset Used:** {model_card['Training Data']['Subset Used']}\n",
    "\n",
    "## Limitations\n",
    "{model_card['Limitations'][0]}\n",
    "{model_card['Limitations'][1]}\n",
    "\n",
    "## Ethical Considerations\n",
    "{model_card['Ethical Considerations'][0]}\n",
    "{model_card['Ethical Considerations'][1]}\n",
    "{model_card['Ethical Considerations'][2]}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e867142c-2b4c-4514-951d-7a72905d8c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write README content to a file\n",
    "with open(\"README.md\", \"w\") as readme_file:\n",
    "    readme_file.write(readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7223d1aa-0b91-479d-b7fd-a00e31729dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md uploaded to ruslanmv/Medical-Mixtral-7B-v2k\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "# Authenticate with Hugging Face \n",
    "api = HfApi()  # Replace with your authentication method\n",
    "# Path to your README.md file\n",
    "current_directory = os.getcwd()\n",
    "# Combine the current directory with the README.md filename\n",
    "readme_path = os.path.join(current_directory, \"README.md\")\n",
    "# Upload the file to the root of the repository (\"\")\n",
    "api.upload_file(\n",
    "    path_or_fileobj=readme_path,\n",
    "    repo_id=repository_name,\n",
    "    path_in_repo=\"README.md\"  # Add this argument\n",
    ")\n",
    "\n",
    "print(f\"README.md uploaded to {repository_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c68b1cf3-61d7-4a1d-ab5f-804942b59368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Save the Model Card\n",
    "model_card_path = os.path.join(save_path, \"model_card.json\")\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    json.dump(model_card, f)\n",
    "\n",
    "model.push_to_hub(new_model, model_card=model_card_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ef00-22ad-4d9e-9df7-4da73be4262e",
   "metadata": {},
   "source": [
    "## Step 11 - Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2996e30e-cdbf-47a1-b929-51bfb004be02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c25c7cf7-277d-4044-b473-06cc96209026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76feda9e-0264-4c0d-9914-ee4baff102d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What does abutment of the nerve root mean?\"\n",
    "prompt = build_prompt(question)\n",
    "result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accb7a62-0636-4d6d-a51a-29b4ccb16a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]@Enlighten. What does abutment of the nerve root mean? [/INST]Hi. I have gone through your query with diligence and would like you to know that I am here to help. For further information consult a neurologist online --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> --> -->\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89eed049-0ef5-465b-8665-5aa3a725de0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_path=\"test.csv\"\n",
    "test_path=\"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "234bddf8-f0b4-438c-a702-a7131ec14456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_test=pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12660cdc-a758-4d79-a0dc-721f16d3d243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Can Yasmin birth control pill be used as an em...</td>\n",
       "      <td>Hi. How are you doing? Yes, as you have heard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>Please explain the poisonous effect of phenol ...</td>\n",
       "      <td>Hi. I want to assure you not to worry as every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>For how long should I take Kalachikai powder t...</td>\n",
       "      <td>Hello. For PCOD (polycystic ovarian disease), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>Why do periods get delayed after first time sex?</td>\n",
       "      <td>Hello. As the serum beta hCG levels are less t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>How to resolve peripheral vision problem?</td>\n",
       "      <td>Hi. Revert back with the report and answers to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "2000  Can Yasmin birth control pill be used as an em...   \n",
       "2001  Please explain the poisonous effect of phenol ...   \n",
       "2002  For how long should I take Kalachikai powder t...   \n",
       "2003   Why do periods get delayed after first time sex?   \n",
       "2004          How to resolve peripheral vision problem?   \n",
       "\n",
       "                                                 answer  \n",
       "2000  Hi. How are you doing? Yes, as you have heard,...  \n",
       "2001  Hi. I want to assure you not to worry as every...  \n",
       "2002  Hello. For PCOD (polycystic ovarian disease), ...  \n",
       "2003  Hello. As the serum beta hCG levels are less t...  \n",
       "2004  Hi. Revert back with the report and answers to...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d65073c7-14a8-4772-94f5-c8d36e97083b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We test only 10 entries\n",
    "df_test=df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c59dc51e-9ca7-4018-9a93-82bd560336d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Can Yasmin birth control pill be used as an em...</td>\n",
       "      <td>Hi. How are you doing? Yes, as you have heard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>Please explain the poisonous effect of phenol ...</td>\n",
       "      <td>Hi. I want to assure you not to worry as every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>For how long should I take Kalachikai powder t...</td>\n",
       "      <td>Hello. For PCOD (polycystic ovarian disease), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>Why do periods get delayed after first time sex?</td>\n",
       "      <td>Hello. As the serum beta hCG levels are less t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>How to resolve peripheral vision problem?</td>\n",
       "      <td>Hi. Revert back with the report and answers to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>Is there any way to treat pinworm infestation ...</td>\n",
       "      <td>Hi. Best is Ivermectin. If resistant, use 5 % ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>What are the ways to get pregnant without pene...</td>\n",
       "      <td>Hi, WeIcome to icliniq.com. I can understand y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>Can I take PPI to treat gurgling caused due to...</td>\n",
       "      <td>Hi. PPI alone will not work. Need to add some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>What should I do to be happy all over the day ...</td>\n",
       "      <td>Hello. First of all, you should stop hating yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>Kindly explain well defined altered cystic sig...</td>\n",
       "      <td>Hello. Regarding your question, I would explai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "2000  Can Yasmin birth control pill be used as an em...   \n",
       "2001  Please explain the poisonous effect of phenol ...   \n",
       "2002  For how long should I take Kalachikai powder t...   \n",
       "2003   Why do periods get delayed after first time sex?   \n",
       "2004          How to resolve peripheral vision problem?   \n",
       "2005  Is there any way to treat pinworm infestation ...   \n",
       "2006  What are the ways to get pregnant without pene...   \n",
       "2007  Can I take PPI to treat gurgling caused due to...   \n",
       "2008  What should I do to be happy all over the day ...   \n",
       "2009  Kindly explain well defined altered cystic sig...   \n",
       "\n",
       "                                                 answer  \n",
       "2000  Hi. How are you doing? Yes, as you have heard,...  \n",
       "2001  Hi. I want to assure you not to worry as every...  \n",
       "2002  Hello. For PCOD (polycystic ovarian disease), ...  \n",
       "2003  Hello. As the serum beta hCG levels are less t...  \n",
       "2004  Hi. Revert back with the report and answers to...  \n",
       "2005  Hi. Best is Ivermectin. If resistant, use 5 % ...  \n",
       "2006  Hi, WeIcome to icliniq.com. I can understand y...  \n",
       "2007  Hi. PPI alone will not work. Need to add some ...  \n",
       "2008  Hello. First of all, you should stop hating yo...  \n",
       "2009  Hello. Regarding your question, I would explai...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fe3d197-4ce1-4f4b-a729-5d9f122bcf87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Can Yasmin birth control pill be used as an emergency contraceptive pill?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I have gone through your query with diligence and would like you to know that I am here to help. For further information consult an obstetrician and gynaecologist online --> http://iclinic.com/gynecologist. I hope I have been able to help. For more information consult an obstetrician and gynaecologist online --> http://iclinic.com/gynecologist. All the best. For more information consult an obstetrician and gynaecologist online --> http://iclinic.com/gynecologist. Thank you for your query. For more information consult an obstetrician and gynaecologist online --> http://iclinic.com\n",
      "correct\n",
      "Progress: 0.1\n",
      "Accuracy: 1.0\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Please explain the poisonous effect of phenol ingestion.\n",
      "\n",
      "LLM Answer:\n",
      "Hi. I want to assure you not to worry as everything is going to be fine if proper care and treatment is opted in for. I have thoroughly gone through your case and can well understand your genuine health concerns. 1. No, there is not much problem right now as he vomited and also had a lot of water. It is fine because, it was only two drops. 2. We usually do not go for emesis (vomiting) for phenol poisoning. Because, it is a volatile compound and causes vapors entering the lungs through the airways. 3. I suggest consulting a general practitioner for physical examination and evaluation. For further follow up consult a general practitioner online.--->>--->--->\n",
      "correct\n",
      "Progress: 0.2\n",
      "Accuracy: 1.0\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "For how long should I take Kalachikai powder to overcome PCOD problem?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. For PCOD (polycystic ovarian syndrome), powder Kalachikai has a good effect, but the duration of treatment is long (more than three months). For proper evaluation of disease, please go for USG whole abdomen and other investigations. If the size and number of cysts are more, then I suggest consulting a gynecologist as you may require surgery. If the number and size of cysts are less, you may continue treatment along with syrup Evecare 10 ml BD and Kanchnar Guggulu two BD. For more information consult an ayurveda specialist online --> --> --> --> --> --> --> --> --> --> --> --> -->\n",
      "correct\n",
      "Progress: 0.3\n",
      "Accuracy: 1.0\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Why do periods get delayed after first time sex?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. As the serum beta hCG levels are less than 3 ng/mL, the pregnancy is definitely ruled out. The delay in the menses is mostly due to sex induced stress after the first time when reproductive hormones readjust themselves. The menses can get delayed even up to two weeks, so just wait for another week and they should arrive. For further information consult an obstetrician and gynaecologist online -->-->-->-->--> http://iclinic.com/gynecologist/online-consultation. For further information consult an obstetrician and gynaecologist online -->-->-->-->--> http://iclinic.com/gynecologist/online-consultation. All the best. For\n",
      "correct\n",
      "Progress: 0.4\n",
      "Accuracy: 1.0\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "How to resolve peripheral vision problem?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I understand your concern. I would like to know the following details: 1. What is the cause of your peripheral vision loss? 2. Is there any other eye problem? 3. Is there any other medical problem? 4. Is there any family history of eye problem? 5. Is there any other symptom? 6. Is there any other eye problem? 7. Is there any other medical problem? 8. Is there any family history of eye problem? 9. Is there any other symptom? 10. Is there any other eye problem? 11. Is there any other medical problem? 12. Is there any family history of eye problem? 13. Is there any other symptom?\n",
      "wrong\n",
      "Progress: 0.5\n",
      "Accuracy: 0.8\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Is there any way to treat pinworm infestation permanently?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I understand your concern. I would like to assure you that I am here to help you. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practitioner online --> http://iclinic.com/consult. For further information consult a general practition\n",
      "correct\n",
      "Progress: 0.6\n",
      "Accuracy: 0.8333333333333334\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "What are the ways to get pregnant without penetration?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I understand your concern. I would like to assure you that you are not alone. I would also like to assure you that you can get pregnant without penetration. I would recommend you to do the following tests: 1. Serum beta hCG (human chorionic gonadotropin). 2. Serum FSH (follicle stimulating hormone). 3. Serum AMH (anti-mullerian hormone). 4. Transvaginal ultrasound. I would also recommend you to do the following: 1. Avoid alcohol. 2. Avoid smoking. 3. Avoid caffeine. 4. Avoid stress. 5. Exercise regularly. 6. E\n",
      "correct\n",
      "Progress: 0.7\n",
      "Accuracy: 0.8571428571428571\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Can I take PPI to treat gurgling caused due to GERD?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I understand your concern. I would recommend you to take the following medications for a month. Consult your specialist doctor, discuss with him or her and take the treatment with their consent. For more information consult a general practitioner online --> http://iclinic.com/consult. Revert with more information to a general practitioner online --> http://iclinic.com/consult. For more information consult a general practitioner online --> http://iclinic.com/consult. For more information consult a general practitioner online --> http://iclinic.com/consult. For more information consult a general practitioner online --> http://iclinic.com/consult. For more information consult a general\n",
      "correct\n",
      "Progress: 0.8\n",
      "Accuracy: 0.875\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "What should I do to be happy all over the day inspite of hurting myself?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. I understand your concern. I would like to suggest you the following: For further information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online --> http://iclinic.com/psychologist. For more information consult a psychiatrist online\n",
      "correct\n",
      "Progress: 0.9\n",
      "Accuracy: 0.8888888888888888\n",
      "#############################\n",
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "Kindly explain well defined altered cystic signal intensity along nerves in MRI.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer:\n",
      "Hello. I understand your concern. I have gone through your query and would like you to know that I am here to help. For further information consult a neurologist online --> http://iclinic.com/consult/neurologist-online. I hope I have clarified your query. For more information consult a neurologist online --> http://iclinic.com/consult/neurologist-online. Thank you for your query. For more information consult a neurologist online --> http://iclinic.com/consult/neurologist-online. All the best. For more information consult a neurologist online --> http://iclinic.com/consult/ne\n",
      "correct\n",
      "Progress: 1.0\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "questionCounter = 0\n",
    "correct = 0\n",
    "promptEnding = \"[/INST]\"\n",
    "\n",
    "# Guide for answering questions\n",
    "testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df_test.iterrows():\n",
    "    print(\"#############################\")\n",
    "    questionCounter += 1\n",
    "\n",
    "    # Build the question prompt\n",
    "    question = testGuide + row['question'] + \"\\n\"\n",
    "    print(question)\n",
    "\n",
    "    # Get the true answer\n",
    "    truth = row['answer']\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    # Generate answer\n",
    "    result = pipe(prompt)\n",
    "    llmAnswer = result[0]['generated_text']\n",
    "\n",
    "    # Remove the prompt from the generated answer\n",
    "    index = llmAnswer.find(promptEnding)\n",
    "    llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "\n",
    "    print(\"LLM Answer:\")\n",
    "    print(llmAnswer)\n",
    "\n",
    "    # Remove spaces from the generated answer\n",
    "    llmAnswer = llmAnswer.replace(' ', '')\n",
    "\n",
    "    # Find the option in response\n",
    "    index = llmAnswer.find('answer:')\n",
    "\n",
    "    # Find and match the option\n",
    "    next_char = llmAnswer[index + len('answer:'):][0]\n",
    "    if next_char in truth:\n",
    "        correct += 1\n",
    "        print('correct')\n",
    "    else:\n",
    "        print('wrong')\n",
    "\n",
    "    # Update accuracy\n",
    "    accuracy = correct / questionCounter\n",
    "    print(f\"Progress: {questionCounter / len(df_test)}\")\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32482f71-23e0-40cc-a1a9-bb1234b680f1",
   "metadata": {},
   "source": [
    "# Medical-Mixtral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7f098-6492-42c4-9fae-0584ed266976",
   "metadata": {},
   "source": [
    "### Testing model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99118e41-4896-4029-84c6-538459b55191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aa506b392846078c4684572a442304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691c1d8663ed4ea3b0aefaf11f619c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdc1f279ecd4e199fe5be0e2c80e99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80591f7738443628a644dd74c17f85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Define the name of your fine-tuned model\n",
    "finetuned_model = \"ruslanmv/Medical-Mixtral-7B-v2k\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b449dc9e-73e8-45d1-9870-9f2a4a12e595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd88644a-4ec6-4150-837d-8f981d2edf60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02391fe7531e4877ad521db7365d25e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec13a14ec1d445ecb24eac3229192c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de87f22b0e7496fa09d2739a2043ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(\n",
    "        finetuned_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd261dfc-dd05-4312-8173-0018af775a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained.eval()\n",
    "## Text Generation Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c58eddf-fac6-4493-867e-ee93e3798013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18d0d154-0519-4738-a1ad-797785788f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model_pretrained, tokenizer=tokenizer, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c532313-b277-4c39-89c2-86fa5c64e10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "  promptEnding = \"[/INST]\"\n",
    "  # Guide for answering questions\n",
    "  testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "  # Build the question prompt\n",
    "  question = testGuide + question + \"\\n\"\n",
    "  print(question)\n",
    "  # Build the prompt\n",
    "  prompt = build_prompt(question)\n",
    "  # Generate answer\n",
    "  result = pipe(prompt)\n",
    "  llmAnswer = result[0]['generated_text']\n",
    "  # Remove the prompt from the generated answer\n",
    "  index = llmAnswer.find(promptEnding)\n",
    "  llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "  print(\"LLM Answer:\")\n",
    "  print(llmAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1dbd3f55-c8f3-468e-8d2b-64be141539fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"For how long should I take Kalachikai powder to overcome PCOD problem?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1280d8d6-8ff9-4117-b1a1-ba592451ff33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question, at the end of your response say thank you for your query.\n",
      "For how long should I take Kalachikai powder to overcome PCOD problem?\n",
      "\n",
      "LLM Answer:\n",
      "Hello. For PCOD (polycystic ovarian syndrome), powder Kalachikai has a good effect, but the duration of treatment is long (more than three months). For proper evaluation of disease, please go for USG\n"
     ]
    }
   ],
   "source": [
    "ask(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
