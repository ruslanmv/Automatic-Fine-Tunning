{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06ce1c4-3d3d-4ad5-9893-27065c290b96",
   "metadata": {},
   "source": [
    "# ruslanmv/Automatic-Fine-Tunning\n",
    "## For SageMaker/ Google Colab\n",
    "\n",
    "In this notebook we are going to Fine Tune the Mixtral Model adapted to ai-medical-chatbot with more than 250k of records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e0b48-12b9-4ca1-8eda-617fe3f21be2",
   "metadata": {},
   "source": [
    "## Step 1 - Identification of Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c419fa98-f56f-42ea-8cdc-a88c9632b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are not on Google Colab.\n",
      "You are on SageMaker notebook instance.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from IPython.core.display import get_ipython\n",
    "  is_colab =  get_ipython() is not None and get_ipython().get_fullname() == '__main__'\n",
    "except:\n",
    "  is_colab = False\n",
    "if is_colab:\n",
    "    print(\"You are on Google Colab!\")\n",
    "else:\n",
    "    print(\"You are not on Google Colab.\")\n",
    "    try:\n",
    "        import boto3\n",
    "        # Assuming you have IAM permissions to list SageMaker notebook instances\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        response = sagemaker_client.list_notebook_instances()\n",
    "        # Check if any notebook instances are listed\n",
    "        if len(response['NotebookInstances']) > 0:\n",
    "            print(\"You are on SageMaker notebook instance.\")\n",
    "            is_sagemaker=True\n",
    "        else:\n",
    "            print(\"SageMaker API check inconclusive.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking with SageMaker API: {e}\")\n",
    "        print(\"Result inconclusive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16083b87-d5a4-452d-baa0-e0622ce87074",
   "metadata": {},
   "source": [
    "## Step 2 - Environment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c1514b-e634-4b18-84ba-e77721b7c9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment name: pytorch_p310\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if is_colab:\n",
    "    #@markdown # Connect Google Drive\n",
    "    from google.colab import drive\n",
    "    from IPython.display import clear_output\n",
    "    import ipywidgets as widgets\n",
    "    import os\n",
    "    def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
    "    Shared_Drive = \"\" #@param {type:\"string\"}\n",
    "    #@markdown - Leave empty if you're not using a shared drive\n",
    "    print(\"[0;33mConnecting...\")\n",
    "    drive.mount('/content/gdrive')\n",
    "    if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "      mainpth=\"Shareddrives/\"+Shared_Drive\n",
    "    else:\n",
    "      mainpth=\"MyDrive\"\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "else:\n",
    "    env_name = os.environ.get(\"CONDA_DEFAULT_ENV\", \"\")\n",
    "    if env_name == \"conda_pytorch_p310\":\n",
    "        print(\"Not detected Default Pytorch Environment\")\n",
    "        print(\"Installing missing packages\")\n",
    "        !pip3 install -qU torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    else:\n",
    "        print(\"Environment name:\", env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ba5c7b-d868-45a9-b1a9-8d002e4cac4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e936faa-d31c-41e0-a4eb-15f94e7970f5",
   "metadata": {},
   "source": [
    "## Step 3 - Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7331fcd-837e-4150-8ee0-6dfa03e6e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if is_colab:\n",
    "    #@markdown # Install/Update ruslanmv repo\n",
    "    from IPython.utils import capture\n",
    "    from IPython.display import clear_output\n",
    "    from subprocess import getoutput\n",
    "    import ipywidgets as widgets\n",
    "    import sys\n",
    "    import fileinput\n",
    "    import os\n",
    "    import time\n",
    "    import base64\n",
    "    import requests\n",
    "    from urllib.request import urlopen, Request\n",
    "    from urllib.parse import urlparse, parse_qs, unquote\n",
    "    from tqdm import tqdm\n",
    "    import six\n",
    "    blsaphemy = base64.b64decode(\"ZWJ1aQ==\").decode('ascii')\n",
    "    if not os.path.exists(\"/content/gdrive\"):\n",
    "        print('\\033[1;31mGdrive not connected, using temporary colab storage ...')\n",
    "        time.sleep(4)\n",
    "        mainpth = \"MyDrive\"\n",
    "        !mkdir -p /content/gdrive/$mainpth\n",
    "        Shared_Drive = \"\"\n",
    "\n",
    "    if Shared_Drive != \"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
    "        print('\\033[1;31mShared drive not detected, using default MyDrive')\n",
    "        mainpth = \"MyDrive\"\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        def inf(msg, style, wdth):\n",
    "            inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth))\n",
    "            display(inf)\n",
    "        fgitclone = \"git clone --depth 1\"\n",
    "        !mkdir -p /content/gdrive/$mainpth/llm\n",
    "        # Define the path\n",
    "        main_path =f\"/content/gdrive/{mainpth}/\"\n",
    "        !git clone -q --branch master https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/$mainpth/llm/Automatic-Fine-Tunning\n",
    "        os.environ['TRANSFORMERS_CACHE'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/cache\"\n",
    "        cache_dir = os.environ['TRANSFORMERS_CACHE']\n",
    "        !mkdir -p /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories\n",
    "        !git clone https://github.com/ruslanmv/Automatic-Fine-Tunning /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "    with capture.capture_output() as cap:\n",
    "        %cd /content/gdrive/{mainpth}/llm/Automatic-Fine-Tunning/{blsaphemy}/repositories/Automatic-Fine-Tunningebui-assets\n",
    "\n",
    "        !git reset --hard\n",
    "        !git checkout master\n",
    "        time.sleep(1)\n",
    "        !git pull\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done', 'success', '50px')\n",
    "    #@markdown ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f82eb4-08a4-4442-995a-effd278ebfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reload_environment():\n",
    "    # Kernel restart logic (may not work consistently within Jupyter Notebook)\n",
    "    try:\n",
    "      from IPython import get_ipython\n",
    "      get_ipython().kernel.do_shutdown(restart=True)\n",
    "      print(\"Kernel restarted. Packages should be reloaded.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Kernel restart failed: {e}\")\n",
    "      print(\"Consider manually restarting the kernel or your Jupyter Notebook server.\")\n",
    "if is_colab:\n",
    "    #@markdown # Requirements\n",
    "    print('[1;32mInstalling requirements...')\n",
    "    with capture.capture_output() as cap:\n",
    "      %cd /content/\n",
    "      !wget -q -i https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt\n",
    "      !pip install -r requirements.txt\n",
    "    clear_output()\n",
    "    inf('\\u2714 Done','success', '50px')\n",
    "    #@markdown ---\n",
    "if is_sagemaker:\n",
    "    !pip install -qU transformers==4.36.2 accelerate==0.25.0 duckduckgo_search==4.1.0  python-dotenv\n",
    "    !pip install -qU  bitsandbytes transformers==4.36.2 peft accelerate trl datasets==2.16.0 sentencepiece protobuf\n",
    "    !wget -q -i https://github.com/ruslanmv/Automatic-Fine-Tunning/raw/master/Dependencies/requirements.txt\n",
    "    !pip install   -qU  -r requirements.txt\n",
    "    #reload_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29228279-6247-43dc-9c69-030192a28322",
   "metadata": {},
   "source": [
    "##  Step 4  - Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5174a2-6cd3-4f18-a9b8-701da2137ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b881bfa-121a-43a3-abd6-d44cd47b131c",
   "metadata": {},
   "source": [
    "## Step 5 - Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db910d7-5b33-42fd-a104-2f968c0cb53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base model from huggingFace or path to model\n",
    "base_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# New model name\n",
    "new_model = \"Medical-Mixtral-7B-v250k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41088a83-f6a9-4820-8241-0400a05113d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF Token\n"
     ]
    }
   ],
   "source": [
    "##  Loading Data\n",
    "# Access the environment variable\n",
    "if is_colab:\n",
    "    from google.colab import userdata\n",
    "    from google.colab import userdata\n",
    "    secret_hf = userdata.get('HF_TOKEN')\n",
    "else:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    # Check if .env file exists\n",
    "    if not os.path.exists('.env'):\n",
    "        # Print the URL for Hugging Face token\n",
    "        print(\"Please go to the following URL and obtain your Hugging Face token:\")\n",
    "        print(\"https://huggingface.co/settings/tokens\")\n",
    "        print()\n",
    "        # Prompt user to enter HF_TOKEN manually\n",
    "        hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "        # Create or append to .env file\n",
    "        with open('.env', 'a') as f:\n",
    "            f.write(f\"HF_TOKEN={hf_token}\\n\")\n",
    "\n",
    "    # Load the .env file\n",
    "    load_dotenv()\n",
    "    # Retrieve the value of HF_TOKEN from the environment variables\n",
    "    secret_hf = os.environ.get('HF_TOKEN')\n",
    "    # Clear output to hide the token\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()\n",
    "    # Print the value of HF_TOKEN\n",
    "    print(\"Loaded HF Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5257211-b39d-4843-ac33-c7a990fde80c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Then you can use the token in your command\n",
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619f3a5-0b94-4b0b-930a-2aa3447e7d7a",
   "metadata": {},
   "source": [
    "## Step 6 -  Datataset to Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93e69ee-b9d1-4431-84bb-6b682cebb212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_ = load_dataset(\"ruslanmv/ai-medical-chatbot\")\n",
    "train_data = dataset_[\"train\"]\n",
    "# For this demo, let's choose the first 256 500 dialogues\n",
    "num=256500\n",
    "\n",
    "df = pd.DataFrame(train_data[::])\n",
    "df = df[[\"Description\", \"Doctor\"]].rename(columns={\"Description\": \"question\", \"Doctor\": \"answer\"})\n",
    "# Clean the question and answer columns\n",
    "df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "# Assuming your DataFrame is named 'df' and the column is named 'df' and the column is named 'question'\n",
    "df['question'] = df['question'].str.lstrip('Q. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5210724-2cd6-4955-8754-11a51b322457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does abutment of the nerve root mean?</td>\n",
       "      <td>Hi. I have gone through your query with dilige...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What should I do to reduce my weight gained du...</td>\n",
       "      <td>Hi. You have really done well with the hypothy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have started to get lots of acne on my face,...</td>\n",
       "      <td>Hi there Acne has multifactorial etiology. Onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do I have uncomfortable feeling between th...</td>\n",
       "      <td>Hello. The popping and discomfort what you fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My symptoms after intercourse threatns me even...</td>\n",
       "      <td>Hello. The HIV test uses a finger prick blood ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0         What does abutment of the nerve root mean?   \n",
       "1  What should I do to reduce my weight gained du...   \n",
       "2  I have started to get lots of acne on my face,...   \n",
       "3  Why do I have uncomfortable feeling between th...   \n",
       "4  My symptoms after intercourse threatns me even...   \n",
       "\n",
       "                                              answer  \n",
       "0  Hi. I have gone through your query with dilige...  \n",
       "1  Hi. You have really done well with the hypothy...  \n",
       "2  Hi there Acne has multifactorial etiology. Onl...  \n",
       "3  Hello. The popping and discomfort what you fel...  \n",
       "4  Hello. The HIV test uses a finger prick blood ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab1ac366-a6f9-4ad2-8f5e-44f328d66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[:num, :]\n",
    "df_test = df.iloc[num:num+100, :]\n",
    "# Save the train dataframe to a CSV file\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "# Save the test dataframe to a CSV file\n",
    "df_test.to_csv('test.csv', index=False)\n",
    "df=df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3405fc-59a1-4c6f-bf08-018f52be3a40",
   "metadata": {},
   "source": [
    "## Step 7 - Formatting Your Fine-tuning Data\n",
    "There are various ways to format your data for fine-tuning\n",
    "\n",
    "Prompts provide context and guide the LLM towards the desired task. The code showcases creating prompts for question-answering tasks with placeholders for questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d12f6a-a784-4cee-a260-37014d689c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build training dataset with the right format\n",
    "df['text'] = '[INST]@Enlighten. ' + df['question'] +'[/INST]'+ df['answer'] + ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21649658-4ea1-488e-9911-928bd159ba95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove columns\n",
    "df=df.drop(['question','answer'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7382ad6-5515-4c7f-a725-58b5c07d43f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST]@Enlighten. What does abutment of the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST]@Enlighten. What should I do to reduce m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST]@Enlighten. I have started to get lots o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST]@Enlighten. Why do I have uncomfortable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST]@Enlighten. My symptoms after intercours...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [INST]@Enlighten. What does abutment of the ne...\n",
       "1  [INST]@Enlighten. What should I do to reduce m...\n",
       "2  [INST]@Enlighten. I have started to get lots o...\n",
       "3  [INST]@Enlighten. Why do I have uncomfortable ...\n",
       "4  [INST]@Enlighten. My symptoms after intercours..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ba91adc-a68a-476b-a17a-ddfdb67c56bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 256500\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataset object\n",
    "dataset = ds.dataset(pa.Table.from_pandas(df).to_batches())\n",
    "dataset = Dataset(pa.Table.from_pandas(df))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef947cf3-5c34-4b8a-8bea-82f364c98b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_sagemaker:\n",
    "    # Get the current directory and join with the models folder\n",
    "    current_directory = os.getcwd()\n",
    "    cache_dir = os.path.join(current_directory, \"models\")\n",
    "    main_path=current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa098e-4f75-47f6-ac0e-7d03323148f1",
   "metadata": {},
   "source": [
    "## Step 8 -  Downloading and Initializing Mixtral 8x7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbae860-d582-474a-b348-babfba9abce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119bed9-3bf8-4ab5-9295-c3a9d9571fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022551-c1f7-4f61-916e-d4efdfad9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, cache_dir=cache_dir)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfe4a6-92cd-450d-87f2-06bea8deb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count trainging tokens\n",
    "from transformers import LlamaTokenizer\n",
    "tokenizer_ = LlamaTokenizer.from_pretrained(\"cognitivecomputations/dolphin-llama2-7b\",\n",
    "                                            cache_dir=cache_dir)\n",
    "tokens = tokenizer_.tokenize(dataset.to_pandas().to_string())\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c3f55-4c99-4234-b9ac-e90c3d76d049",
   "metadata": {},
   "source": [
    "## Step 9 - Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ce2be-67aa-4de2-8391-7dab57836edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the adapters in the layers\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c7e38-6127-4b89-9c14-635fc3875de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b64b8-ec90-4689-9f7c-2a4a07940a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ed137-84d3-49ec-9535-e7e24e23d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13cf914-bf95-47ea-acbe-f4d7a400e374",
   "metadata": {},
   "source": [
    "## Step 10 - Save and push the adapter to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c6395b-c4cb-49a6-99f5-ccca98774d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model name\n",
    "new_model = \"Medical-Mixtral-7B-v250k\"\n",
    "# Save the fine-tuned model\n",
    "save_path = os.path.join(main_path, \"llm\", \"Automatic-Fine-Tuning\", \"models\", new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed70892-1533-4a1a-9f8c-b4accdcd74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(save_path)\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ec690-9265-4360-8042-8fca1ea1dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84390f-36a6-4587-991c-4d9014bfea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895ff6a-7210-443d-a02f-da43f5bb3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a Model Card\n",
    "model_card = {\n",
    "  \"Model Name\": \"Medical-Mixtral-7B-250k\",\n",
    "  \"Description\": \"Fine-tuned Mixtral model for answering medical assistance questions. This model is a novel version of mistralai/Mixtral-8x7B-Instruct-v0.1, adapted to a subset of 1.5k records from the AI Medical Chatbot dataset, which contains 250k records (https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot). The purpose of this model is to provide a ready chatbot to answer questions related to medical assistance.\",\n",
    "  \"Intended Use\": \"This model is intended for providing assistance and answering questions related to medical inquiries. It is suitable for use in chatbot applications where users seek medical advice, information, or assistance.\",\n",
    "  \"Example Usage\": \"```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Define the name of your fine-tuned model\\nfinetuned_model = 'ruslanmv/Medical-Assistance-Mixtral-7B-v1.5k'\\n\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(finetuned_model, trust_remote_code=True)\\n\\n# Load the model with the provided adapter configuration and weights\\nmodel_pretrained = AutoModelForCausalLM.from_pretrained(finetuned_model, trust_remote_code=True, torch_dtype=torch.float16, cache_dir=cache_dir)\\n\\nmessages = [\\n    {'role': 'user', 'content': 'What should I do to reduce my weight gained due to genetic hypothyroidism?'},\\n    {'role': 'assistant', 'content': ''},\\n]\\n\\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\\n\\noutputs = model_pretrained.generate(input_ids, max_new_tokens=500)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\",\n",
    "  \"Training Data\": {\n",
    "    \"Dataset Name\": \"AI Medical Chatbot\",\n",
    "    \"Dataset URL\": \"https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot\",\n",
    "    \"Dataset Size\": \"250k records\",\n",
    "    \"Subset Used\": \"250k records\"\n",
    "  },\n",
    "  \"Limitations\": [\n",
    "    \"The model's performance may vary depending on the complexity and specificity of the medical questions.\",\n",
    "    \"The model may not provide accurate answers for every medical query, and users should consult medical professionals for critical healthcare concerns.\"\n",
    "  ],\n",
    "  \"Ethical Considerations\": [\n",
    "    \"Users should be informed that the model's responses are generated based on patterns in the training data and may not always be accurate or suitable for medical decision-making.\",\n",
    "    \"The model should not be used as a replacement for professional medical advice or diagnosis.\",\n",
    "    \"Sensitive patient data should not be shared with the model, and user privacy should be protected.\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Save the Model Card\n",
    "model_card_path = os.path.join(save_path, \"model_card.json\")\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    json.dump(model_card, f)\n",
    "\n",
    "model.push_to_hub(new_model, model_card=model_card_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ef00-22ad-4d9e-9df7-4da73be4262e",
   "metadata": {},
   "source": [
    "## Step 11 - Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996e30e-cdbf-47a1-b929-51bfb004be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c7cf7-277d-4044-b473-06cc96209026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feda9e-0264-4c0d-9914-ee4baff102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does abutment of the nerve root mean?\"\n",
    "prompt = build_prompt(question)\n",
    "result = pipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb7a62-0636-4d6d-a51a-29b4ccb16a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eed049-0ef5-465b-8665-5aa3a725de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_path=\"test.csv\"\n",
    "test_path=\"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bddf8-f0b4-438c-a702-a7131ec14456",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test=pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12660cdc-a758-4d79-a0dc-721f16d3d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65073c7-14a8-4772-94f5-c8d36e97083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We test only 10 entries\n",
    "df_test=df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc51e-9ca7-4018-9a93-82bd560336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3d197-4ce1-4f4b-a729-5d9f122bcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionCounter = 0\n",
    "correct = 0\n",
    "promptEnding = \"[/INST]\"\n",
    "\n",
    "# Guide for answering questions\n",
    "testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df_test.iterrows():\n",
    "    print(\"#############################\")\n",
    "    questionCounter += 1\n",
    "\n",
    "    # Build the question prompt\n",
    "    question = testGuide + row['question'] + \"\\n\"\n",
    "    print(question)\n",
    "\n",
    "    # Get the true answer\n",
    "    truth = row['answer']\n",
    "\n",
    "    # Build the prompt\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    # Generate answer\n",
    "    result = pipe(prompt)\n",
    "    llmAnswer = result[0]['generated_text']\n",
    "\n",
    "    # Remove the prompt from the generated answer\n",
    "    index = llmAnswer.find(promptEnding)\n",
    "    llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "\n",
    "    print(\"LLM Answer:\")\n",
    "    print(llmAnswer)\n",
    "\n",
    "    # Remove spaces from the generated answer\n",
    "    llmAnswer = llmAnswer.replace(' ', '')\n",
    "\n",
    "    # Find the option in response\n",
    "    index = llmAnswer.find('answer:')\n",
    "\n",
    "    # Find and match the option\n",
    "    next_char = llmAnswer[index + len('answer:'):][0]\n",
    "    if next_char in truth:\n",
    "        correct += 1\n",
    "        print('correct')\n",
    "    else:\n",
    "        print('wrong')\n",
    "\n",
    "    # Update accuracy\n",
    "    accuracy = correct / questionCounter\n",
    "    print(f\"Progress: {questionCounter / len(df_test)}\")\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76bd5e-e8e4-43c0-b4c4-195c15fb757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Medical-Mixtral-7B-v1.5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978cf0f-1297-4da2-9250-b834ffa0af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99118e41-4896-4029-84c6-538459b55191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Define the name of your fine-tuned model\n",
    "finetuned_model = \"ruslanmv/Medical-Mixtral-7B-v250k\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model,\n",
    "                                          trust_remote_code=True,\n",
    "                                          cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449dc9e-73e8-45d1-9870-9f2a4a12e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88644a-4ec6-4150-837d-8f981d2edf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(\n",
    "        finetuned_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd261dfc-dd05-4312-8173-0018af775a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pretrained.eval()\n",
    "## Text Generation Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c58eddf-fac6-4493-867e-ee93e3798013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question):\n",
    "  prompt=f\"[INST]@Enlighten. {question} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0d154-0519-4738-a1ad-797785788f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model_pretrained, tokenizer=tokenizer, max_length=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c532313-b277-4c39-89c2-86fa5c64e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "  promptEnding = \"[/INST]\"\n",
    "  # Guide for answering questions\n",
    "  testGuide = 'Answer the following question, at the end of your response say thank you for your query.\\n'\n",
    "  # Build the question prompt\n",
    "  question = testGuide + question + \"\\n\"\n",
    "  print(question)\n",
    "  # Build the prompt\n",
    "  prompt = build_prompt(question)\n",
    "  # Generate answer\n",
    "  result = pipe(prompt)\n",
    "  llmAnswer = result[0]['generated_text']\n",
    "  # Remove the prompt from the generated answer\n",
    "  index = llmAnswer.find(promptEnding)\n",
    "  llmAnswer = llmAnswer[len(promptEnding) + index:]\n",
    "  print(\"LLM Answer:\")\n",
    "  print(llmAnswer)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd3f55-c8f3-468e-8d2b-64be141539fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"My symptoms after intercourse threatns me even after having negative HIV result. Kindly help.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280d8d6-8ff9-4117-b1a1-ba592451ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
